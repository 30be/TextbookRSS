https://lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents
https://lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities
https://lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem
https://lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction
https://lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai
https://lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer
https://lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge
https://lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default
https://lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target
https://lesswrong.com/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign
https://lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1
https://lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control
https://lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty
https://lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans
https://lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into
https://lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior
https://lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment
https://lesswrong.com/posts/ZyWyAJbedvEgRT2uF/inaccessible-information
https://lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators
https://lesswrong.com/posts/bBdfbWfWxHN9Chjcq/robustness-to-scale
https://lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-convergently-instrumental-in-mdps
https://lesswrong.com/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition
https://lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq
https://lesswrong.com/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption
https://lesswrong.com/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated
https://lesswrong.com/posts/xCxeBSHqMEaP3jDvY/reframing-impact
https://lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent
https://lesswrong.com/posts/AanbbjYr5zckMKde7/specification-gaming-examples-in-ai-1
https://lesswrong.com/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions
https://lesswrong.com/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth
https://lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence
https://lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents
https://lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails
https://lesswrong.com/posts/EF5M6CmKRd6qZk27Z/my-research-methodology
https://lesswrong.com/posts/rP66bz34crvDudzcJ/decision-theory-does-not-imply-that-we-get-to-have-nice
https://lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1
https://lesswrong.com/posts/r3NHPD3dLFNk9QE2Y/search-versus-design-1
https://lesswrong.com/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free
https://lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing
https://lesswrong.com/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next
https://lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking
https://lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal
https://lesswrong.com/posts/mRwJce3npmzbKfxws/efficientzero-how-it-works
https://lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward
https://lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget
https://lesswrong.com/posts/Ke2ogqSEhL2KCJCNx/security-mindset-lessons-from-20-years-of-software-security
https://lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation
https://lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities
https://lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization
https://lesswrong.com/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide
https://lesswrong.com/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about
https://lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values
https://lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without
